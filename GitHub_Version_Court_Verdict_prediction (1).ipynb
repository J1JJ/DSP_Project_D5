{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "6hV6n2E1hGBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of necessary imports\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer\n",
        "import torch\n",
        "from transformers import RobertaForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.nn.functional import softmax\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "vk2pY6--hJGS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "-H7SJ_FkiRbz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWbO08Z8Y1o8"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive') # dataset stored in google drive, can also be a local file\n",
        "df = pd.read_excel('/content/drive/MyDrive/DSP/punishmentextraction.xlsx') # pd.read_csv if dataset was downloaded in that format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Arr_TD2EU32q"
      },
      "outputs": [],
      "source": [
        "# depending on different verdict prediction, different columns of the df should be selected, Guilty vs not Guilty is selected as an example\n",
        "court_description = df[\"Court Description\"].tolist()\n",
        "labels = df[\"Guilty Binary\"].tolist()\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    court_description, labels, test_size=0.2, stratify=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data(Chunking)"
      ],
      "metadata": {
        "id": "1sRRMyrXkE22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mChE-Bm8hIGJ"
      },
      "outputs": [],
      "source": [
        "# Uncomment the model to be run\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d3oJjJ9alJRf"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, tokenizer, max_length=512):\n",
        "  '''\n",
        "  Function for chunking the court cases\n",
        "\n",
        "  Arrgs:\n",
        "    text - court case\n",
        "    tokernizer - tokenizer\n",
        "    max_length - maximum length of the court case\n",
        "  '''\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSG1q-3flKE2"
      },
      "outputs": [],
      "source": [
        "chunked_train_texts = []\n",
        "chunked_train_labels = []\n",
        "train_chunks_per_text = []\n",
        "\n",
        "for text, label in zip(train_texts, train_labels):\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "    chunked_train_texts.extend(chunks)\n",
        "    chunked_train_labels.extend([label] * len(chunks))\n",
        "    train_chunks_per_text.append(len(chunks))\n",
        "\n",
        "\n",
        "chunked_test_texts = []\n",
        "chunked_test_labels = []\n",
        "test_chunks_per_text = []\n",
        "\n",
        "for text, label in zip(test_texts, test_labels):\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "    chunked_test_texts.extend(chunks)\n",
        "    chunked_test_labels.extend([label] * len(chunks))\n",
        "    test_chunks_per_text.append(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bZXE6pcohLPI"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(chunked_train_texts, truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(chunked_test_texts, truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8YjgKApbhMea"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O2pcCbDzhZC1"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(train_encodings, chunked_train_labels)\n",
        "test_dataset = Dataset(test_encodings, chunked_test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "weHYe8Fyka2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JlBWyaRhZvt"
      },
      "outputs": [],
      "source": [
        "# Uncomment the model to be tested\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/bert-base-dutch-cased\", num_labels=2)\n",
        "# model = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2QYdUmqhgjS"
      },
      "outputs": [],
      "source": [
        "# Hyperparametsrs\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    log_level=\"info\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "UbbRkkJfkgrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyn-orFBhmP8"
      },
      "outputs": [],
      "source": [
        "# An Automatic Weights & Biases logging might be required, provied your own\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "Gia-KWedki-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Weky2roIhprP"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ciWjbSrJUq"
      },
      "outputs": [],
      "source": [
        "predictions_output = trainer.predict(test_dataset)\n",
        "logits = predictions_output.predictions\n",
        "probabilities = softmax(torch.tensor(logits), dim=1)\n",
        "probs = probabilities[:, 1].tolist()\n",
        "\n",
        "# assembling the court cases from the chunks and thier predictions\n",
        "start_idx = 0\n",
        "chunk_predictions = []\n",
        "for num_chunks in test_chunks_per_text:\n",
        "    chunk_predictions.append(probs[start_idx : start_idx + num_chunks])\n",
        "    start_idx += num_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6_elF6svrq8O"
      },
      "outputs": [],
      "source": [
        "def aggregate_predictions(predictions, strategy):\n",
        "  '''\n",
        "  Function for specifying the way the results are calculated.\n",
        "\n",
        "  Arrgs:\n",
        "    predictions - list of predictions\n",
        "    strategy - max, mean, min\n",
        "  '''\n",
        "\n",
        "    if strategy == \"max\":\n",
        "        return max(predictions)\n",
        "    elif strategy == \"mean\":\n",
        "        return sum(predictions) / len(predictions)\n",
        "    elif strategy == \"min\":\n",
        "        return min(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xTRPQGierrnY"
      },
      "outputs": [],
      "source": [
        "aggregated_predictions = [aggregate_predictions(preds, \"mean\") for preds in chunk_predictions]\n",
        "\n",
        "binary_predictions = [1 if prob >= 0.5 else 0 for prob in aggregated_predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVQnElMYz1s8"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix for calculating the precision and recall and accaracy\n",
        "cm = confusion_matrix(test_labels, binary_predictions)\n",
        "cm"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}